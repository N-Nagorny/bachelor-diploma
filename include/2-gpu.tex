\section{Неспециализированные вычисления на графических процессорах}

Прообразом первых графических процессоров, появившихся в 90-е годы XX века, были специализированные чипы аркадных автоматов [1]. Их использование было обусловлено малыми объёмами оперативной памяти, что не позволяло хранить в ней кадры перед отправкой на устройство видеовывода. В дальнейшем разделение вычислений на графические и неграфические лишь усилилось, что оказало существенное влияние на архитектуру современных компьютеров.

В начале XXI века графические процессоры получили поддержку шейдеров и возможность работы с числами с плавающей запятой. Это событие положило начало ряду экспериментов с организацией неграфических параллельных расчётов на графических процессорах. При помощи графических API данные передавались в виде текстур, а расчётные программы --- в виде шейдеров [2]. Таким образом учёные начали производить вычисления, связанные с матрицами и векторами, на GPU. Первой программой, выполнившейся заметно быстрее на GPU, чем на CPU, стала реализация LU-разложения (2005).

С увеличением популярности использования GPU для научных расчётов начали формироваться идеи фреймворков общего назначения, позволяющих отойти от графической парадигмы работы с данными и отказаться от использования OpenGL или DirectX. Такими фреймворками впоследствии стали технологии  CUDA и OpenCL.

\subsection{Краткий обзор технологии NVIDIA CUDA}

Первоначальная версия CUDA SDK была представлена 15 февраля 2007 г. В основе CUDA API лежит диалект языка C [3].

Компиляция CUDA-программ выполняется специализированным компилятором nvcc. При этом код программ разделяется на host-часть, выполняющуюся CPU, и device-часть, выполняющуюся графическим процессором. В результате получаются как минимум два объектных файла, готовых к сборке в конечный исполняемый файл в любой среде программирования [4].

По сравнению с использовавшимся ранее подходом к организации вычислений общего назначения посредством возможностей графических API, архитектура CUDA имеет ряд преимуществ:
\begin{itemize}
\item использование диалекта языка C, что позволяет упростить процесс изучения архитектуры;
\item полная аппаратная поддержка целочисленных и побитовых операций;
\item разделяемая между потоками память размером в 16 Кбайт может быть использована под организованный пользователем кэш с более широкой полосой пропускания, чем при выборке из обычных текстур.
\end{itemize}

\subsection{Краткий обзор технологии OpenCL}

OpenCL --- фреймворк для написания компьютерных программ, связанных с параллельными вычислениями на различных графических и центральных процессорах, DSP, FPGA и прочих процессорах и аппаратных ускорителях [5]. При работе с OpenCL API так же используется диалект языка C [6].

Основной задачей проекта OpenCL является создание и поддержка открытого стандарта, позволяющего создавать универсальные программы для параллельных вычислений на различных процессорах и вычислительные машины, использующие несколько процессоров различных архитектур одновременно.

\subsection{Иерархия потоков выполнения в NVIDIA CUDA}

Как упоминалось ранее, одной из особенностей написания программ с использованием технологии CUDA является разделение всего программного кода на host- и device-части. Для этого используются спецификаторы функций:
\texttt{\_\_host\_\_} --- означает, что данный код предназначен для центрального процессора (используется по умолчанию ); \\
\texttt{\_\_device\_\_} --- означает, что данный код работает на видеокарте ; \\
\texttt{\_\_global\_\_} --- особый спецификатор для так называемых функций-ядер (kernel), которые запускаются с центрального процессора, а работают на видеокарте.

Остановимся подробнее на функциях-ядрах. Их отличие от обычных функций языка C заключается в том, что при вызове они выполняются N раз параллельно в N потоках выполнения. При этом количество потоков выпол-нения, которые можно создать на GPU, практически не ограничено.

Для организации работы со столь большим количеством потоков ис-пользуется иерархическая структура: потоки объединяются в варпы (warp), варпы, в свою очередь, — в блоки (block), а блоки составляют сетку (grid).

Варп — это минимальная независимая от других единица выполнения кода. Размер варпа всегда равен 32 потокам. Эти потоки всегда выполняются физически синхронно. Блок — это автономная группа потоков. Взаимодей-ствия потоков между блоками невозможны.

Вызов функции-ядра осуществляется следующим образом:
\begin{verbatim}
kernel_name<<<grid_size, block_size>>>(arguments);
\end{verbatim}
В тройных угловых скобках в участке программного кода выше указы-ваются размеры сетки и блока.
Примером может послужить программа для перемножения матриц: она иллюстрирует параллельное выполнения потоков с помощью многократного повторения операции сложения с разными числами.
\begin{verbatim}
__global__ void MatAdd(
    float A[N][N], float B[N][N], float C[N][N])
{
    int i = threadIdx.x;
    int j = threadIdx.y;
    C[i][j] = A[i][j] + B[i][j];
}

int main()
{
    ...
    int numBlocks = 1;
    dim3 threadsPerBlock(N, N);
    MatAdd<<<numBlocks, threadsPerBlock>>>(A, B, C);
    ...
}

\end{verbatim}
Здесь threadIdx — трёхкомпонентный вектор, хранящий координаты по-тока в блоке (рис. 2). Таким образом, видим: имеется один блок размеров N×N и три массива той же размерности. Каждому потоку ставится в соответ-ствие по одному элементу каждой из трёх матриц. Следовательно, все потоки покрывают все элементы матриц. Блок выполняет перемножение матриц как одну операцию благодаря параллельной работе потоков, работающих всего лишь с тремя элементами матриц по отдельности.
\subsection{Иерархия памяти в NVIDIA CUDA}
Выделяют следующие виды памяти видеокарты:
\begin{itemize}
\item глобальная память — аналог оперативной памяти ПК и основной вид памяти, используемый для обмена данными между CPU и GPU;
\item разделяемая память — управляемый программно L1-кэш (64 Кбайт на каждый потоковый мультипроцессор);
\item регистровая локальная память — память, из которой выделяются реги-стры для каждого потока (64 000 регистров на каждый потоковый мульти-процессор);
\item дополнительная локальная память — участок глобальной памяти, вы-деляемый потоку при нехватке регистров;
\item константная память — память, модификация которой возможна только с CPU;
\item текстурная память — интерфейс чтения глобальной памяти с использо-ванием специфических для текстур операций [7].
\end{itemize}
Для каждого потока выделяется локальная память (регистровая и, опци-онально, дополнительная). Для каждого блока выделяется разделяемая па-мять, доступ к которой имеют все потоки блока [8]. Разделяемая память вы-делена, пока существует блок. Все существующие потоки имеют доступ к глобальной памяти. Также каждый поток имеет доступ к чтению константной и текстурной памяти. Глобальная, константная и текстурная память не пере-выделяются на протяжении работы программы, т. о. разные функции-ядра могут работать с этими видами памяти.

\subsection{Иерархия потоков выполнения в OpenCL}
Иерархическая модель потоков выполнения OpenCL концептуально не отличается от своего аналога от компании NVIDIA.
Аналогом потоков в OpenCL являются рабочие элементы (work-item). Они объединяются в рабочие группы (work-group), причём рабочие элемен-ты, принадлежащие разным рабочим группам, не могут взаимодействовать [9].
На рис. 4 изображён пример двухмерного пространства индексов (2D-range index space), являющееся аналогом сетки в NVIDIA CUDA. В общем случае пространство индексов является N-мерным [10].
Данное пространство индексов разделено на 16 рабочих групп, имею-щих собственные координаты и размер 8×8. Рабочие элементы имеют два типа координат: локальные (относительно рабочей группы) и глобальные (относительно сетки целиком).
По аналогии с CUDA используются функции-ядра, выполняющиеся на OpenCL-устройстве.
\subsection{Иерархия памяти в OpenCL}
Так как хост-машина и OpenCL-устройство не имеют общего адресного пространства, взаимодейстие между памятью хоста и памятью OpenCL-устройства происходит посредством использования различных областей па-мяти.
Глобальная память — это участок памяти, к которому все рабочие эле-менты, группы, а также хост, имеют полный доступ (чтение и запись). Эта область памяти может быть выделена только хостом.
Константная память — это участок глобальной памяти, остающийся не-тронутым на протяжении выполнения функции-ядра. Рабочие группы могут только читать данные из этой области, хост же имеет к ней полный доступ.
Локальная память — это место обмена данными между рабочими эле-ментами рабочей группы. Все элементы имеют полный доступ к этой обла-сти.
Внутренняя память — это память, принадлежащая конкретному рабо-чему элементу [11].
В большинстве случаев память хоста и память OpenCL-устройства рабо-тают независимо друг от друга. Соответственно, имеется специфика обмена данными между ними: необходимо перемещать данные из памяти хоста в глобальную память, затем в локальную, и обратно.
\subsection{Специфика работы с видеопамятью}
Перед организацией параллельных вычислений следует ознакомиться с некоторыми приёмами оптимизации, которые помогут сохранить производи-тельность, выигранную от использования GPU. Наиболее широко распро-странённым и лёгким в освоении приёмом является согласованный доступ к памяти, аналогичный выравниванию памяти при работе CPU с оперативной памятью.
Глобальная память физически расположена на device-устройстве, доступ к которой происходит посредством 32-, 64- и 128-байтных транзакций. Не-обходимое условие для их существования — обращение к выровненным участкам памяти. Таковыми являются участки, адреса первых элементов ко-торых кратны 32, 64 и 128 байтам соответственно.
Когда варп выполняет инструкцию, обращающуюся к глобальной памя-ти, обращения каждого потока внутри варпа сливаются в одну или несколько транзакций, в зависимости от размера слова, запрашиваемого каждым пото-ком, и разброса адресов, к которым обращаются потоки.
Команды работы с глобальной памятью поддерживают чтение и запись слов размером 1, 2, 4, 8 и 16 байтов. Любой запрос к глобальной памяти возможен только в случае, если размер запрашиваемых данных ра-вен 1, 2, 4, 8 или 16 байтам, и они выровнены (т. е. их адреса кратны разме-ру).
Затем одиночные запросы к глобальной памяти объединяются в тран-закции. Например, запросы 32 потоков одного варпа, обращающиеся к че-тырём последовательно располагающимся байтам каждый, объединятся в транзакцию размером 128 байтов в том случае, если адрес, к которому об-ращается первый поток варпа, кратен 128 байтам.